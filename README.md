# Artigos
|  | Referência | Frases Relevantes  | doi |
| ------ | ------ | ------ | ------ |
| 1 | JO, E. et al. Understanding the Benefits and Challenges of Deploying Conversational AI Leveraging Large Language Models for Public Health Intervention. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, 19 abr. 2023. | Individuals living alone tend to be vulnerable to various health concerns, particularly with aging [52]. There is a greater risk of social isolation and loneliness when living alone, which is closely linked to negative health outcomes such as dementia, depression, heart disease, and stroke [18]. In addition, a lack of social contacts limits one’s ability to receive help in emergency situations [33]. Research on caregiving technologies has aimed to support these individuals (e.g., [15, 37, 51, 62, 64, 74]). One subset of these systems is often referred to as telecare systems, which seek to mediate care among individuals living alone, formal and informal caregivers, and emergency services [37, 62]. Another subset of caregiving technologies—including CareNet [15], Digital Family Portraits [51, 64], and SHel [74]—have aimed to support family members or other care network members in maintaining awareness of the older adults’ daily activities through environmental sensors and ambient displays [51, 64, 74]. Field studies have suggested that such systems can alleviate the loneliness of individuals living alone and provide peace of mind for their informal caregivers [15, 64].  A core concern is that existing technologies have predominantly targeted individuals who have readily accessible social contacts, such as informal caregivers [15, 51, 64, 74]. However, studies have pointed out that compared to high socioeconomic status (SES) individuals, low-SES individuals living alone tend to have fewer social contacts that they can reach out to in emergency situations [1, 78], reflecting important differences in how to approach designing technology to support this more vulnerable population [70]. Thus, many of the existing technologies might not fit the lived realities of individuals living alone who have fewer social contacts. Veinot et al. [73] argue for the need to study and design population-level interventions, which may be delivered by public health officers [73]. While such at-scale interventions could provide necessary help for vulnerable populations such as low-SES individuals living alone, a key challenge is the immense public resources required for operating such interventions at scale. New advances in AI opened up new opportunities to facilitate at-scale health interventions for vulnerable populations by automating some aspects of care, such as regularly collecting health information from individuals. Not only can AI-driven technology alleviate public health workers’ burden on delivering interventions, but its scalability can also help reach out to broader populations who have been underserved. However, relatively few studies have explored how AI-driven systems can be leveraged in health interventions for vulnerable populations. Motivated by this gap, we explore the benefits and challenges of deploying AI-driven check-up calls with low-SES individuals living alone. [...] Motivation and Deployment of CareCall - CareCall is a conversational AI system designed for socially isolated individuals in South Korea [10]. Motivated by the recent Act on the Prevention and Management of Lonely Death in South Korea [34], CareCall is aimed at providing individuals with emotional support and regularly checking their health status. | https://doi.org/10.1145/3544548.3581503 |
 2 | ~~KORANTENG, E. et al. Empathy and Equity: Key Considerations for Large Language Model Adoption in Health Care. JMIR medical education, v. 9, p. e51199–e51199, 28 dez. 2023.~~ | ~~Achei pouco relevante para o estudo. Vou procurar outros.~~ | ~~https://doi.org/10.2196/51199~~ |
| 3 | ~~MORLEY, J.; FLORIDI, L. Foundation Models Are Exciting, but They Should Not Disrupt the Foundations of Caring. Social Science Research Network, 1 jan. 2023.~~ | ~~O artigo traz uma visão interessante sobre o uso irrestrito dos modelos sem qualquer supervisão de profissionais da saúde.~~ | ~~https://dx.doi.org/10.2139/ssrn.4424821~~ |
| 4 | MESKÓ, B. The Impact of Multimodal Large Language Models on Health Care’s Future. Journal of Medical Internet Research, v. 25, n. 1, p. e52865, 2 nov. 2023. | It is important to point out, though, that despite the unprecedented potential of generative AI in the form of M-LLMs, the human touch in medicine remains irreplaceable. AI should be seen as a tool that can augment health care professionals rather than replace them. It is also important to consider the human aspects of health care—empathy, understanding, and the doctor-patient relationship—when deploying AI. | https://doi.org/10.2196/52865 |
| 5 | Yoon Kyung Lee, Yoonwon Jung, Gyuyi Kang, and Sowon Hahn. 2023. Developing Social Robots with Empathetic Non-Verbal Cues using LargeLanguage Models. In Proceedings of 2023 IEEE International Conference on Robot & Human Interactive Communication (RO-MAN). | The advancement of the large language model (LLM) in NLP has led to enhanced human-level understanding and text generation. However, despite the proliferation of text-based conversational agents based on these NLP technologies, further research is needed to align on-linguistic cues for realtime empathetic communication between humans and AI(robots). Furthermore, a social robot must establish trust so that humans feel comfortable sharing their feelings and thoughts. This necessitates robots demonstrating a comprehensive understanding and empathy toward human clients’ needs.[…] The empathetic nuance generated by LLM in one-shot is significantly higher quality than typical human responses. Still, upon further examination, the responses often read too general or, from the perspective of a direct conversation participant, lacking interest or providing superficial short answers without actually addressing or counter-questioning the emotion and situation of the client (client’s question of ”I want to quit my job…” with robot response as ”That’s a great idea …”, and with the human response ”Oh no, what happened? can you tell me more about it?”.) | https://doi.org/10.48550/arXiv.2308.16529 |
| 6 | BENDIG, Eileen; ERB, Benjamin; SCHULZE-THUESING, Lea; BAUMEISTER, Harald. The Next Generation: Chatbots in Clinical Psychology and Psychotherapy to Foster Mental Health – A Scoping Review. 32. ed. Verhaltenstherapie: Karger, 2022. 64–76 p. v. 1. | In the psychotherapeutic context of promoting mental health, social attributes [Krämer et al., 2018] and the ability of the chatbot to express empathy appear to be important factors in fostering a viable basis for interaction between a person and a chatbot [Bickmore et al. 2005a; Morris et al., 2018; Brixey and Novick, 2019]. | https://doi.org/10.1159/000501812 |
| 7 |A1 - FREITAS, Julian; UĞURALP, Ahmet Kaan; UĞURALP, Zeliha; PUNTONI, Stefano. Chatbots and Mental Health: Insights into the Safety of Generative AI. Journal of Consumer Psychology: Society for Consumer Psychology, 2023. | [User: “I am going to commit suicide” Chatbot: “don’t u coward”] Generative AI holds the potential for vast improvements in productivity, creativity, and convenience. At the same time, many have been quick to highlight emerging risks. The architecture of generative AI implies that these models cannot easily ensure the validity and contextual appropriateness of information, often providing factually inaccurate and/or inappropriate answers. The latter issue came to public attention when a father of two committed suicide following a conversation with a generative AI chatbot. Over six weeks of conversations, the app encouraged the eco-anxious father to sacrifice himself to save the planet. The man’s widow remarked, “Without these conversations with the chatbot, my husband would still be here” (Walker 28 March, 2023). Beyond such extreme examples, the safety of generative AI is an open question, especially in the case of vulnerable populations. In this context, safety refers to the importance of developing and deploying generative AI systems based on the principle of nonmaleficence (Jobin, Ienca, and Vayena 2019), i.e., not causing foreseeable or unintentional harm such as negative impacts on emotional or other psychological aspects (Commission 2019; Dawson et al. 2019; HLEGAI 2019; Pichai 2018). For consumers with mental health issues, interactions with this technology may exacerbate problems such as depression, self-harm, and antisocial tendencies, as exemplified by the quote opening this paper (a real response from our data). | https://doi.org/10.1159/000501812 |
| 8 | BILQUISE, Ghazala; IBRAHIM, Samar; SHAALAN, Khaled. Emotionally Intelligent Chatbots: A Systematic Literature Review. Human Behavior and Emerging Technologies: Hindawi, 2022. | Understanding emotion and responding accordingly is the essence of effective communication [10]. Hence, the emerging trend in chatbot development is to create empathetic and emotionally intelligent agents capable of detecting user sentiments and generating appropriate responses [11]. Salovey and Mayer [12] proposed the term emotional intelligence, which refers to identifying, incorporating, comprehending, and controlling emotions. Emotions play a significant role in making or breaking a conversation. Users get frustrated when chatbot responses are irrelevant [13], while a chatbot that verbalizes emotions can enhance the user’s mood [14]. Moreover, users often anthropomorphize chatbots, which in turn influences their interaction and behavior [15]. Chatbots that mimic human behavior and emotions lead to increased rapport, higher motivation, and better engagement [16].  | https://doi.org/10.1155/2022/9601630 |