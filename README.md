# Artigos
|  | Referência | Comentário  | doi |
| ------ | ------ | ------ | ------ |
| 1 | JO, E. et al. Understanding the Benefits and Challenges of Deploying Conversational AI Leveraging Large Language Models for Public Health Intervention. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, 19 abr. 2023. | O CareCall ajuda a gerenciar a solidão e a carga emocional. Tanto teleoperadores quanto usuários relataram que as chamadas regulares do sistema proporcionam uma sensação de companhia e apoio emocional. Usuários solitários e emocionalmente sobrecarregados valorizam as conversas atenciosas com o chatbot, que muitas vezes são mais confortáveis e menos exigentes emocionalmente do que interações humanas. As conversas não se limitam à saúde, abrangendo tópicos como hobbies e vida cultural, o que melhora o bem-estar emocional dos usuários. A ausência de carga emocional nas interações com o chatbot é considerado boa, pois os usuários não sentem a pressão de recusar convites ou preocupações com o trabalho dos agentes humanos. Isso torna as chamadas do CareCall uma forma eficaz de apoio emocional sem sobrecarregar os operadores de teleoperadores. | https://doi.org/10.1145/3544548.3581503 |
 2 | ~~KORANTENG, E. et al. Empathy and Equity: Key Considerations for Large Language Model Adoption in Health Care. JMIR medical education, v. 9, p. e51199–e51199, 28 dez. 2023.~~ | Achei pouco relevante para o estudo. Vou procurar outros. | https://doi.org/10.2196/51199 |
| 3 | MORLEY, J.; FLORIDI, L. Foundation Models Are Exciting, but They Should Not Disrupt the Foundations of Caring. Social Science Research Network, 1 jan. 2023. | O artigo traz uma visão interessante sobre o uso irrestrito dos modelos sem qualquer supervisão de profissionais da saúde. | https://dx.doi.org/10.2139/ssrn.4424821 |
| 4 | MESKÓ, B. The Impact of Multimodal Large Language Models on Health Care’s Future. Journal of Medical Internet Research, v. 25, n. 1, p. e52865, 2 nov. 2023. | À medida que os modelos se integram às práticas de saúde, é essencial abordar as limitações, implicações éticas e legais, como a privacidade dos dados e a confiabilidade das informações geradas pelos M-LLMs, para garantir o uso seguro e responsável. Foi também ressaltado que, apesar do potencial sem precedentes dos M-LLMs, o toque humano na medicina permanece insubstituível. Os modelos devem ser vistos como uma ferramenta que pode auxiliar os profissionais de saúde, e não substituí-los. Ao implantar os LLMs, é crucial considerar os aspectos humanos do cuidado de saúde, como empatia, compreensão e a relação médico-paciente. | https://doi.org/10.2196/52865 |
| 5 | Yoon Kyung Lee, Yoonwon Jung, Gyuyi Kang, and Sowon Hahn. 2023. Developing Social Robots with Empathetic Non-Verbal Cues using LargeLanguage Models. In Proceedings of 2023 IEEE International Conference on Robot & Human Interactive Communication (RO-MAN). | No contexto da interação humano-robô (HRI), a empatia é vital para a construção de relacionamentos e comunicação eficaz, especialmente em áreas como tutoria, aconselhamento e colaboração. Avanços no reconhecimento de emoções e empatia robótica têm melhorado essas interações, embora a incorporação de sinais não-verbais continue sub-representada. Integrar pistas sociais não-verbais em modelos de grandes linguagens (LLMs) empáticos poderia melhorar ainda mais a HRI, alinhando-se melhor com as preferências humanas e promovendo interações mais naturais e eficazes. | https://doi.org/10.48550/arXiv.2308.16529 |
| 6 | BENDIG, Eileen; ERB, Benjamin; SCHULZE-THUESING, Lea; BAUMEISTER, Harald. The Next Generation: Chatbots in Clinical Psychology and Psychotherapy to Foster Mental Health – A Scoping Review. 32. ed. Verhaltenstherapie: Karger, 2022. 64–76 p. v. 1. | In the psychotherapeutic context of promoting mental health, social attributes [Krämer et al., 2018] and the ability of the chatbot to express empathy appear to be important factors in fostering a viable basis for interaction between a person and a chatbot [Bickmore et al. 2005a; Morris et al., 2018; Brixey and Novick, 2019]. | https://doi.org/10.1159/000501812 |
| 7 |A1 - FREITAS, Julian; UĞURALP, Ahmet Kaan; UĞURALP, Zeliha; PUNTONI, Stefano. Chatbots and Mental Health: Insights into the Safety of Generative AI. Journal of Consumer Psychology: Society for Consumer Psychology, 2023. | [User: “I am going to commit suicide” Chatbot: “don’t u coward”] Generative AI holds the potential for vast improvements in productivity, creativity, and convenience. At the same time, many have been quick to highlight emerging risks. The architecture of generative AI implies that these models cannot easily ensure the validity and contextual appropriateness of information, often providing factually inaccurate and/or inappropriate answers. The latter issue came to public attention when a father of two committed suicide following a conversation with a generative AI chatbot. Over six weeks of conversations, the app encouraged the eco-anxious father to sacrifice himself to save the planet. The man’s widow remarked, “Without these conversations with the chatbot, my husband would still be here” (Walker 28 March, 2023). Beyond such extreme examples, the safety of generative AI is an open question, especially in the case of vulnerable populations. In this context, safety refers to the importance of developing and deploying generative AI systems based on the principle of nonmaleficence (Jobin, Ienca, and Vayena 2019), i.e., not causing foreseeable or unintentional harm such as negative impacts on emotional or other psychological aspects (Commission 2019; Dawson et al. 2019; HLEGAI 2019; Pichai 2018). For consumers with mental health issues, interactions with this technology may exacerbate problems such as depression, self-harm, and antisocial tendencies, as exemplified by the quote opening this paper (a real response from our data). | https://doi.org/10.1159/000501812 |